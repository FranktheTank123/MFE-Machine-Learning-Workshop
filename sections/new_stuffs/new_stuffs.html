<!DOCTYPE html><html>

<head>
<meta charset="utf-8">
<title>index</title>
<style>
@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

body {
  background-color: white;
}

.markdown-body {
  min-width: 200px;
  max-width: 760px;
  margin: 0 auto;
  padding: 20px;

  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;;word-wrap: break-word; word-break: break-all;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body strong {
  font-weight: bold;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px/1.4 Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code {
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .anchor {
  position: absolute;
  top: 0;
  bottom: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  display: none;
  color: #000;
  vertical-align: middle;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  height: 1em;
  padding-left: 8px;
  margin-left: -30px;
  line-height: 1;
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  display: inline-block;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

.markdown-body .highlight {
  background: #fff;
}

.markdown-body .highlight .mf,
.markdown-body .highlight .mh,
.markdown-body .highlight .mi,
.markdown-body .highlight .mo,
.markdown-body .highlight .il,
.markdown-body .highlight .m {
  color: #945277;
}

.markdown-body .highlight .s,
.markdown-body .highlight .sb,
.markdown-body .highlight .sc,
.markdown-body .highlight .sd,
.markdown-body .highlight .s2,
.markdown-body .highlight .se,
.markdown-body .highlight .sh,
.markdown-body .highlight .si,
.markdown-body .highlight .sx,
.markdown-body .highlight .s1 {
  color: #df5000;
}

.markdown-body .highlight .kc,
.markdown-body .highlight .kd,
.markdown-body .highlight .kn,
.markdown-body .highlight .kp,
.markdown-body .highlight .kr,
.markdown-body .highlight .kt,
.markdown-body .highlight .k,
.markdown-body .highlight .o {
  font-weight: bold;
}

.markdown-body .highlight .kt {
  color: #458;
}

.markdown-body .highlight .c,
.markdown-body .highlight .cm,
.markdown-body .highlight .c1 {
  color: #998;
  font-style: italic;
}

.markdown-body .highlight .cp,
.markdown-body .highlight .cs {
  color: #999;
  font-weight: bold;
}

.markdown-body .highlight .cs {
  font-style: italic;
}

.markdown-body .highlight .n {
  color: #333;
}

.markdown-body .highlight .na,
.markdown-body .highlight .nv,
.markdown-body .highlight .vc,
.markdown-body .highlight .vg,
.markdown-body .highlight .vi {
  color: #008080;
}

.markdown-body .highlight .nb {
  color: #0086B3;
}

.markdown-body .highlight .nc {
  color: #458;
  font-weight: bold;
}

.markdown-body .highlight .no {
  color: #094e99;
}

.markdown-body .highlight .ni {
  color: #800080;
}

.markdown-body .highlight .ne {
  color: #990000;
  font-weight: bold;
}

.markdown-body .highlight .nf {
  color: #945277;
  font-weight: bold;
}

.markdown-body .highlight .nn {
  color: #555;
}

.markdown-body .highlight .nt {
  color: #000080;
}

.markdown-body .highlight .err {
  color: #a61717;
  background-color: #e3d2d2;
}

.markdown-body .highlight .gd {
  color: #000;
  background-color: #fdd;
}

.markdown-body .highlight .gd .x {
  color: #000;
  background-color: #faa;
}

.markdown-body .highlight .ge {
  font-style: italic;
}

.markdown-body .highlight .gr {
  color: #aa0000;
}

.markdown-body .highlight .gh {
  color: #999;
}

.markdown-body .highlight .gi {
  color: #000;
  background-color: #dfd;
}

.markdown-body .highlight .gi .x {
  color: #000;
  background-color: #afa;
}

.markdown-body .highlight .go {
  color: #888;
}

.markdown-body .highlight .gp {
  color: #555;
}

.markdown-body .highlight .gs {
  font-weight: bold;
}

.markdown-body .highlight .gu {
  color: #800080;
  font-weight: bold;
}

.markdown-body .highlight .gt {
  color: #aa0000;
}

.markdown-body .highlight .ow {
  font-weight: bold;
}

.markdown-body .highlight .w {
  color: #bbb;
}

.markdown-body .highlight .sr {
  color: #017936;
}

.markdown-body .highlight .ss {
  color: #8b467f;
}

.markdown-body .highlight .bp {
  color: #999;
}

.markdown-body .highlight .gc {
  color: #999;
  background-color: #EAF2F5;
}

.markdown-body .octicon {
  font: normal normal 16px octicons-anchor;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.markdown-body .octicon-link:before {
  content: '\f05c';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  float: left;
  margin: 0.3em 0 0.25em -1.6em;
  vertical-align: middle;
}
table td{ word-wrap: break-word !important; word-break: break-all !important; }
/*

github.com style (c) Vasily Polovnyov <vast@whiteants.net>

*/

.hljs {
  display: block;
  overflow-x: auto;
  padding: 0.5em;
  color: #333;
  background: #f8f8f8;
  -webkit-text-size-adjust: none;
}

.hljs-comment,
.diff .hljs-header {
  color: #998;
  font-style: italic;
}

.hljs-keyword,
.css .rule .hljs-keyword,
.hljs-winutils,
.nginx .hljs-title,
.hljs-subst,
.hljs-request,
.hljs-status {
  color: #333;
  font-weight: bold;
}

.hljs-number,
.hljs-hexcolor,
.ruby .hljs-constant {
  color: #008080;
}

.hljs-string,
.hljs-tag .hljs-value,
.hljs-doctag,
.tex .hljs-formula {
  color: #d14;
}

.hljs-title,
.hljs-id,
.scss .hljs-preprocessor {
  color: #900;
  font-weight: bold;
}

.hljs-list .hljs-keyword,
.hljs-subst {
  font-weight: normal;
}

.hljs-class .hljs-title,
.hljs-type,
.vhdl .hljs-literal,
.tex .hljs-command {
  color: #458;
  font-weight: bold;
}

.hljs-tag,
.hljs-tag .hljs-title,
.hljs-rule .hljs-property,
.django .hljs-tag .hljs-keyword {
  color: #000080;
  font-weight: normal;
}

.hljs-attribute,
.hljs-variable,
.lisp .hljs-body,
.hljs-name {
  color: #008080;
}

.hljs-regexp {
  color: #009926;
}

.hljs-symbol,
.ruby .hljs-symbol .hljs-string,
.lisp .hljs-keyword,
.clojure .hljs-keyword,
.scheme .hljs-keyword,
.tex .hljs-special,
.hljs-prompt {
  color: #990073;
}

.hljs-built_in {
  color: #0086b3;
}

.hljs-preprocessor,
.hljs-pragma,
.hljs-pi,
.hljs-doctype,
.hljs-shebang,
.hljs-cdata {
  color: #999;
  font-weight: bold;
}

.hljs-deletion {
  background: #fdd;
}

.hljs-addition {
  background: #dfd;
}

.diff .hljs-change {
  background: #0086b3;
}

.hljs-chunk {
  color: #aaa;
}


</style>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>
<style> @media print{ .hljs{overflow: visible; word-wrap: break-word !important;} }</style></head><body><div class="markdown-body">
<h1 id="toc_0">Learn Something New</h1>

<h2 id="toc_1">Supervised Learning -- Revisited</h2>

<p>A <strong>supervised</strong> model has following parts:</p>

<ul>
<li><strong>Data</strong>: \(X\) and \(Y\)</li>
<li><strong>Parameters</strong>: some variables you are trying to learn/fit/optimize.</li>
<li><strong>Hyper-parameters</strong>: some <strong>pre-set</strong> macros which controls model complexity and/or model behavior. For example

<ul>
<li><strong>Polynomial regression</strong>: order \(n\)</li>
<li><strong>Multi-dimensional linear regression</strong>: <a href="https://en.wikipedia.org/wiki/Kernel_regression">kernels</a>, regularization parameter \(\lambda\), regularization type L1/L2</li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor"><strong>Random Forest</strong></a>: <code>n_estimators</code>, <code>max_depth</code>, etc.</li>
<li><strong>Neural Network</strong>: learning rate, activation functions, drop-out ratio, etc.</li>
</ul></li>
<li><strong>Algorithm</strong>: i.e., which model? <strong>A hyper-parameter</strong>. Examples: 

<ul>
<li><a href="https://en.wikipedia.org/wiki/Generalized_additive_model">Additive</a> or multiplicative</li>
<li><a href="https://en.wikipedia.org/wiki/Decision_tree">Decision tree</a></li>
<li><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support vector machine</a></li>
<li>Ensembles</li>
<li>...</li>
</ul></li>
<li><strong>Loss function</strong>: <strong>Also a hyper-parameter</strong>. Common loss function:

<ul>
<li><a href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean square error</a>: regression</li>
<li><a href="https://en.wikipedia.org/wiki/Mean_absolute_error">Mean absolute error</a>: regression</li>
<li><a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross entropy</a>: classification</li>
<li>negative log-<a href="https://en.wikipedia.org/wiki/Likelihood_function">Likelihood function</a>: probabilistic models</li>
<li><a href="https://en.wikipedia.org/wiki/Gramian_matrix">Gram matrix</a>: <a href="http://genekogan.com/works/style-transfer/">style transfer</a></li>
<li><a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">Regularization</a> (can be add to any loss)

<ul>
<li><strong>AIC/BIC</strong>: \(||\beta||_0\) for simpliness</li>
<li><strong>L1</strong>: \(||\beta||_1\) for sparse-ness</li>
<li><strong>L2</strong>: \(||\beta||_2\) for small, non-zero coefficient</li>
<li><strong>Early stopping</strong>: regularization in time</li>
<li><strong>Share parameter</strong>: e.g., recurrent/convolutional network</li>
<li>...</li>
</ul></li>
</ul></li>
<li><strong>Optimizer</strong>: <strong>Again, a hyper-parameter</strong>. How to find the best numerical solutions? For example:

<ul>
<li>Random guess</li>
<li>Closed-form solution</li>
<li>Gradient-based optimization</li>
<li><a href="https://en.wikipedia.org/wiki/Genetic_algorithm">Genetic algorithm</a></li>
</ul></li>
</ul>

<h2 id="toc_2">Long-term Forecasting in Machine Learning World</h2>

<p><strong>Question</strong>: Given a time-series \(y_t\), What if we have want to forecast \(H\) steps further in the future?</p>

<h3 id="toc_3">Seasonal ARIMA?</h3>

<ul>
<li>well-suited for short-term forecasts, not for longer term forecasts</li>
<li>convergence of the autoregressive part</li>
</ul>

<p><img src="images/ARIMA_longterm.png" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>￼</p>

<h3 id="toc_4">Let&#39;s use ML</h3>

<p>Still assume <strong>assume</strong> \(y_t\) follows some additive autoregressive models: </p>

<p>\[y_{t+1} = f(y_t, ..., y_{t-n+1}) + \epsilon_t\]</p>

<ul>
<li><strong>Note I didn&#39;t assume stationarity here.</strong> (Why?)</li>
<li>\(f(\cdot)\) can be any machine learning model with

<ul>
<li>\(X = [\vec y_t, ..., \vec y_{t-n+1}]\)</li>
<li>\(Y = \vec y_{t+1}\)</li>
<li>To be more specific:
\[X =  \begin{bmatrix}
y_t     &amp; y_{t-1} &amp; \dots  &amp; y_{t-n+1} \\
y_{t-1} &amp; y_{t-2} &amp; \dots  &amp; y_{t-n}   \\
\vdots  &amp; \vdots &amp; \ddots &amp; \vdots \\
y_n     &amp; y_{n-1} &amp; \dots  &amp; y_1
\end{bmatrix},
Y =  \begin{bmatrix}
y_{t+1}  \\
y_{t}   \\
\vdots \\
y_{n+1}
\end{bmatrix} \\
\]</li>
<li>Using <a href="http://scikit-learn.org/">Sklearn</a> syntax: <code>f = model.fit(X, Y)</code></li>
</ul></li>
<li>When \(H=1\), any ML models can take care of.</li>
<li>When \(H&gt;1\), things become more interesting. Three possible solutions presented here.</li>
<li><strong>Assume \(n=2\) from now on.</strong></li>
</ul>

<h4 id="toc_5">Solution 1: Iterated forecasting</h4>

<p>We forecast \(y\)&#39;s one at a time.<br/>
\[ <br/>
\hat y_{t+1} = f(y_t, y_{t-1}) \\<br/>
\hat y_{t+2} = f(y_{t+1}, y_{t}) \\<br/>
\vdots \\<br/>
\hat y_{t+H} = f(y_{t+H-1}, y_{t+H-2}) <br/>
\]</p>

<p><img src="images/sol11.png" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>￼</p>

<p><strong>However, we are standing at time \(t\). We don&#39;t know anything at \(t+1\)!</strong> </p>

<p>We need to replace the future by our estimates!</p>

<p>\[ <br/>
\hat y_{t+1} = f(y_t, y_{t-1}) \\<br/>
\hat y_{t+2} = f(\hat y_{t+1}, y_{t}) \\<br/>
\vdots \\<br/>
\hat y_{t+H} = f(\hat y_{t+H-1}, \hat y_{t+H-2}) <br/>
\]</p>

<p><img src="images/sol12.png" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>￼</p>

<ul>
<li><strong>An unbiased estimator of</strong> \(\mathbb{E}[y_{t+1:(t+H)}|y_t]\), since it preserves the stochastic dependencies of the underlying data. </li>
<li><strong>Bias-variance trade-off</strong>: suffers from high variance due to the <strong>accumulation of error in the individual forecasts</strong>. </li>
<li>Low performance over longer time horizons H.</li>
<li>When we have additional inputs, \(x_t\), we need to forecast \(\hat x_{t+h}\) as well!</li>
</ul>

<h4 id="toc_6">Solution 2: \(H\)-step ahead forecasting</h4>

<p>Learn \(H\) different models:<br/>
\[<br/>
\hat y_{t+1} = f_1(y_t, y_{t-1}) \\<br/>
\hat y_{t+2} = f_1(y_t, y_{t-1}) \\<br/>
\vdots \\<br/>
\hat y_{t+H} = f_H(y_t, y_{t-1})<br/>
\]</p>

<p><img src="images/sol2.png" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>￼</p>

<ul>
<li>Does NOT suffer from the accumulation of error.</li>
<li><strong>Models are trained independently</strong>, no statistical dependencies between the predicted values \(y_{t+h}\) are guaranteed.</li>
</ul>

<h4 id="toc_7">Solution 3: Multiple input multiple output (MIMO) models</h4>

<p>One model fits all.<br/>
\[[\hat y_{t+H}, \dots ,\hat y_{t+1}] = f(y_t, y_{t-1})\]</p>

<ul>
<li>No conditional independence assumptions are made.</li>
<li>No accumulation of error of individual forecasts.</li>
<li>All horizons \(H\) are forecasted with the same model, which <strong>limits flexibility</strong>. </li>
</ul>

<h4 id="toc_8">Summary</h4>

<ul>
<li>No free lunch. </li>
<li>Going to traditional ML means no uncertainty estimates.</li>
<li>Work-around: <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Block_bootstrap">bootstrap</a> or Bayesian regression (computationally $$)</li>
</ul>

<h3 id="toc_9">How to decide \(n\)?</h3>

<ul>
<li><strong>Small \(n\)</strong>: simpler model, restricted explainability. Unlikely to capture the full seasonality.</li>
<li><strong>Big \(n\)</strong>: complex model, easy to overfit, don&#39;t know where to stop.</li>
<li>\(n\) usually is a hyper-parameter to tune.</li>
<li>Or, use a <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent model</a> (i.e., \(n=\infty\))</li>
</ul>

<p><img src="images/rr.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>￼</p>

<h2 id="toc_10">Hyperparameter Tuning</h2>

<p><strong>The most time-costly thing you will ever encounter in ML!</strong></p>

<h3 id="toc_11">Model training</h3>

<p>When you hear</p>

<blockquote>
<p>Let&#39;s train a model ... -- Your future boss</p>
</blockquote>

<p>it means:</p>

<ol>
<li>Choose a set of hyper-parameter: 

<ul>
<li><strong>Regularizer</strong> \(\lambda\)</li>
<li><strong>Model</strong> \(f(\cdot)\)</li>
<li><strong>Loss</strong> \(L(y, \hat y)\)</li>
<li><strong>Optimizer</strong></li>
</ul></li>
<li>Prepare dataset, \(X\) and \(Y\)</li>
<li>A supervised model, with parameters \(\theta\), can be thus defined as
\[\hat Y = f_{\lambda}(X, \theta)\]</li>
<li>Use optimizer to solve
\[\theta^* = \text{argmin}_{\theta} L(Y, \hat Y)\]</li>
</ol>

<h3 id="toc_12">Model evaluations</h3>

<p>Hyper-parameter space is (almost) infinite and non-convex. There will always be a better model:</p>

<ul>
<li>Impossible to achieve global maximum</li>
<li>Gradient-base method cannot be used at hyper-parameter level (not always true -- 
<a href="https://arxiv.org/abs/1606.04474">Learning to learn by gradient descent by gradient descent</a>)</li>
</ul>

<p>Given we have some models (with their own hyper-parameters), <strong>how do we compare them?</strong></p>

<ul>
<li>Define an evaluation metric

<ul>
<li>Sharpe Ratio</li>
<li>PnL</li>
<li>Accuracies</li>
<li>Click-through rate (recommendation system)</li>
<li>ETA (dispatch system)</li>
<li>...</li>
</ul></li>
<li>Train these models on a training set</li>
<li>Evaluate on a validation set</li>
<li>Pick the best model(s) with best performance on the validation set</li>
<li>(Optional) Re-train the model(s) on train + validation set</li>
</ul>

<p><strong>Question</strong>: How do we choose finite models out of the infinite model domain?</p>

<h3 id="toc_13">Time is money</h3>

<p>Some benchmarks of training a model (i.e., a set of hyper-parameter) with &lt; 10G of data:</p>

<ul>
<li><strong>Linear regressions</strong>: gradient method, parallelizable, &lt;1min</li>
<li><strong>Random forest</strong>: gradient method, parallelizable, ~10min</li>
<li><strong>Boostings</strong>: gradient method, cannot be parallelized, &lt;1h</li>
</ul>

<p>Above have well-defined functional forms. What if</p>

<ul>
<li>\(f(\cdot)\) is explicitly unknown and multimodal.</li>
<li>Evaluations of \(f(\cdot)\) may be perturbed (non-convex).</li>
<li>Evaluations of \(f(\cdot)\) are expensive. </li>
</ul>

<p>Such as </p>

<ul>
<li><strong>Neural networks</strong>: hours ~ days
<img src="images/NN_tune.png" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>￼</li>
<li><strong>Dispatch algo</strong>: hours
<img src="images/dispatch_tune.png" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>￼</li>
<li><strong>A/B Testing</strong>: days
<img src="images/ab_tune.png" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>￼￼</li>
<li><strong>Design of experiments: gene optimization</strong>: years?
<img src="images/gene_tune.png" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>￼</li>
</ul>

<p><strong>What are we aiming for</strong>: Get a <strong>good-enough model</strong> with as fewer try as possible</p>

<h3 id="toc_14">Option 1: Use previous knowledge</h3>

<p>To select the parameters at hand. Perhaps not very scientific but still in use...</p>

<h3 id="toc_15">Option 2: Grid search</h3>

<ul>
<li>A brute force way to iterate through all possibilities.</li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV">Sklearn API</a></li>
<li>How to grid search?

<ul>
<li>discrete variables: simple iterate</li>
<li>continuous variable

<ul>
<li>uniform grid (e.g., hidden dimensions)</li>
<li>log grid (e.g., learning rate)</li>
</ul></li>
</ul></li>
<li><strong>Curse of dimensionality!</strong> </li>
</ul>

<h3 id="toc_16">Option 3: Random search</h3>

<ul>
<li>Some hyper-parameters are useless (won&#39;t improve model performace)</li>
<li>Better than grid search in various senses but still expensive to guarantee good coverage.</li>
</ul>

<p><img src="images/random_search.png" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>￼</p>

<p><strong>Question</strong>: Can we do better?</p>

<h3 id="toc_17">Option 4: Bayesian optimization</h3>

<p>Given fixed data-set, \(X\) and \(Y\), and pre-specified evaluation metric, \(L[f_\lambda(x, \theta), y]\), <strong>hyper-parameter and model performance is a mapping</strong>.</p>

<p><strong>Goal</strong>: fit a function (i.e., another model), \(g: \lambda \to L(f_\lambda(X_{val}, \theta), Y_{val})\)</p>

<ul>
<li>\(g\) is a non-parametric <strong>meta-model</strong>.</li>
<li>We can only afford very few &quot;training data&quot; (i.e., hyper-param search) to fit \(g\) -- <strong>Bayesian models are better</strong>.</li>
<li><strong>State-of-art</strong>: <a href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian Process</a></li>
<li><strong>Illustration</strong> <a href="https://www.iro.umontreal.ca/%7Ebengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf">here</a>

<ul>
<li><strong>Simple idea</strong>: p8-17</li>
<li><strong>Simple algo</strong>: p34-40</li>
<li><strong>Why doesn’t everyone use this</strong>: p41</li>
</ul></li>
</ul>

<h2 id="toc_18">Proper Backtesting</h2>

<blockquote>
<p>A <strong>quantitative trading strategy</strong> is indeed a <strong>hyper-parameter</strong> -- Frank Xia<br/>
<strong>Backtesting</strong>: validation/hyper-param tuning through time -- Frank Xia</p>
</blockquote>

<ul>
<li>How to split data <strong>correctly</strong> (Draw on white board)</li>
<li>Retrain on train+val is a <strong>must</strong> (when is not a must?)</li>
<li>Robust backtesting

<ul>
<li><strong>Rolling backtest</strong>: how to avoid <strong>beginner&#39;s luck</strong>?</li>
<li><strong>Model ensemble</strong>: how to reduce variance and seasonality? -- what do ensemble?</li>
</ul></li>
</ul>

<blockquote>
<p>It&#39;s a miracle when <strong>loss function</strong> and <strong>evaluation metric</strong> match. -- Frank Xia</p>
</blockquote>

<ul>
<li>To pick a proper loss function -- an art or a science? </li>
</ul>

</div></body>

</html>
